{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(8) # for reproduce\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "import random\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#then import my own modules\n",
    "# from AttentiveFP import Fingerprint, Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight\n",
    "from timeit import default_timer as timer\n",
    "from AttentiveFP.featurizing import graph_dict\n",
    "from AttentiveFP.AttentiveLayers_new import Fingerprint, graph_dataset, null_collate, Graph, Logger, time_to_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_aviable = torch.cuda.is_available()\n",
    "device = torch.device(0)\n",
    "\n",
    "SEED = 8\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if cuda_aviable:\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  9999\n",
      "number of successfully processed smiles:  9999\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'Malaria Bioactivity'\n",
    "tasks = ['Loge EC50']\n",
    "\n",
    "raw_filename = \"../data/malaria-processed.csv\"\n",
    "feature_filename = raw_filename.replace('.csv','.pickle')\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename, names = [\"Loge EC50\", \"smiles\"])\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \",len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        print(smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)].reset_index()\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 888 # 69ï¼Œ103, 107\n",
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "batch_size = 80\n",
    "epochs = 200\n",
    "\n",
    "p_dropout= 0.2\n",
    "fingerprint_dim = 128\n",
    "\n",
    "weight_decay = 5 # also known as l2_regularization_lambda\n",
    "learning_rate = 2.5\n",
    "radius = 2\n",
    "T = 2\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph dicts loaded from ../data/malaria-processed.pkl\n"
     ]
    }
   ],
   "source": [
    "smiles_list = smiles_tasks_df['smiles'].values\n",
    "label_list = smiles_tasks_df[tasks[0]].values\n",
    "graph_dict = graph_dict(smiles_list, label_list, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=88)\n",
    "train_fold = []\n",
    "valid_fold = []\n",
    "for k, (train_idx, valid_idx) in enumerate(kfold.split(smiles_list)):\n",
    "    train_fold.append(train_idx)\n",
    "    valid_fold.append(valid_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823687\n",
      "preprocess.0.linear.weight torch.Size([128, 39])\n",
      "preprocess.0.linear.bias torch.Size([128])\n",
      "preprocess.0.bn.weight torch.Size([128])\n",
      "preprocess.0.bn.bias torch.Size([128])\n",
      "propagate.0.encoder.0.linear.weight torch.Size([128, 138])\n",
      "propagate.0.encoder.0.linear.bias torch.Size([128])\n",
      "propagate.0.encoder.0.bn.weight torch.Size([128])\n",
      "propagate.0.encoder.0.bn.bias torch.Size([128])\n",
      "propagate.0.align.weight torch.Size([1, 256])\n",
      "propagate.0.align.bias torch.Size([1])\n",
      "propagate.0.attend.linear.weight torch.Size([128, 128])\n",
      "propagate.0.attend.linear.bias torch.Size([128])\n",
      "propagate.0.attend.bn.weight torch.Size([128])\n",
      "propagate.0.attend.bn.bias torch.Size([128])\n",
      "propagate.0.gru.weight_ih torch.Size([384, 128])\n",
      "propagate.0.gru.weight_hh torch.Size([384, 128])\n",
      "propagate.0.gru.bias_ih torch.Size([384])\n",
      "propagate.0.gru.bias_hh torch.Size([384])\n",
      "propagate.1.encoder.0.linear.weight torch.Size([128, 138])\n",
      "propagate.1.encoder.0.linear.bias torch.Size([128])\n",
      "propagate.1.encoder.0.bn.weight torch.Size([128])\n",
      "propagate.1.encoder.0.bn.bias torch.Size([128])\n",
      "propagate.1.align.weight torch.Size([1, 256])\n",
      "propagate.1.align.bias torch.Size([1])\n",
      "propagate.1.attend.linear.weight torch.Size([128, 128])\n",
      "propagate.1.attend.linear.bias torch.Size([128])\n",
      "propagate.1.attend.bn.weight torch.Size([128])\n",
      "propagate.1.attend.bn.bias torch.Size([128])\n",
      "propagate.1.gru.weight_ih torch.Size([384, 128])\n",
      "propagate.1.gru.weight_hh torch.Size([384, 128])\n",
      "propagate.1.gru.bias_ih torch.Size([384])\n",
      "propagate.1.gru.bias_hh torch.Size([384])\n",
      "propagate.2.encoder.0.linear.weight torch.Size([128, 138])\n",
      "propagate.2.encoder.0.linear.bias torch.Size([128])\n",
      "propagate.2.encoder.0.bn.weight torch.Size([128])\n",
      "propagate.2.encoder.0.bn.bias torch.Size([128])\n",
      "propagate.2.align.weight torch.Size([1, 256])\n",
      "propagate.2.align.bias torch.Size([1])\n",
      "propagate.2.attend.linear.weight torch.Size([128, 128])\n",
      "propagate.2.attend.linear.bias torch.Size([128])\n",
      "propagate.2.attend.bn.weight torch.Size([128])\n",
      "propagate.2.attend.bn.bias torch.Size([128])\n",
      "propagate.2.gru.weight_ih torch.Size([384, 128])\n",
      "propagate.2.gru.weight_hh torch.Size([384, 128])\n",
      "propagate.2.gru.bias_ih torch.Size([384])\n",
      "propagate.2.gru.bias_hh torch.Size([384])\n",
      "superGather.0.align.weight torch.Size([1, 256])\n",
      "superGather.0.align.bias torch.Size([1])\n",
      "superGather.0.attend.linear.weight torch.Size([128, 128])\n",
      "superGather.0.attend.linear.bias torch.Size([128])\n",
      "superGather.0.attend.bn.weight torch.Size([128])\n",
      "superGather.0.attend.bn.bias torch.Size([128])\n",
      "superGather.0.gru.weight_ih torch.Size([384, 128])\n",
      "superGather.0.gru.weight_hh torch.Size([384, 128])\n",
      "superGather.0.gru.bias_ih torch.Size([384])\n",
      "superGather.0.gru.bias_hh torch.Size([384])\n",
      "superGather.1.align.weight torch.Size([1, 256])\n",
      "superGather.1.align.bias torch.Size([1])\n",
      "superGather.1.attend.linear.weight torch.Size([128, 128])\n",
      "superGather.1.attend.linear.bias torch.Size([128])\n",
      "superGather.1.attend.bn.weight torch.Size([128])\n",
      "superGather.1.attend.bn.bias torch.Size([128])\n",
      "superGather.1.gru.weight_ih torch.Size([384, 128])\n",
      "superGather.1.gru.weight_hh torch.Size([384, 128])\n",
      "superGather.1.gru.bias_ih torch.Size([384])\n",
      "superGather.1.gru.bias_hh torch.Size([384])\n",
      "superGather.2.align.weight torch.Size([1, 256])\n",
      "superGather.2.align.bias torch.Size([1])\n",
      "superGather.2.attend.linear.weight torch.Size([128, 128])\n",
      "superGather.2.attend.linear.bias torch.Size([128])\n",
      "superGather.2.attend.bn.weight torch.Size([128])\n",
      "superGather.2.attend.bn.bias torch.Size([128])\n",
      "superGather.2.gru.weight_ih torch.Size([384, 128])\n",
      "superGather.2.gru.weight_hh torch.Size([384, 128])\n",
      "superGather.2.gru.bias_ih torch.Size([384])\n",
      "superGather.2.gru.bias_hh torch.Size([384])\n",
      "predict.0.linear.weight torch.Size([512, 128])\n",
      "predict.0.linear.bias torch.Size([512])\n",
      "predict.0.bn.weight torch.Size([512])\n",
      "predict.0.bn.bias torch.Size([512])\n",
      "predict.3.weight torch.Size([1, 512])\n",
      "predict.3.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(num_target=output_units_num, fingerprint_dim=128, K=3, T=3)\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "# optimizer = optim.SGD(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(smiles_list):\n",
    "    model.train()\n",
    "    train_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=True)\n",
    "    losses = []\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(train_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        \n",
    "        loss = loss_function(mol_prediction, label.view(-1,1))     \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "        \n",
    "def eval(smiles_list):\n",
    "    model.eval()\n",
    "    eval_MAE_list = []\n",
    "    eval_MSE_list = []\n",
    "    eval_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=False)\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(eval_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "#         print([i for i in zip(mol_prediction, label)])\n",
    "        MAE = F.l1_loss(mol_prediction, label.view(-1,1), reduction='none')        \n",
    "        MSE = F.mse_loss(mol_prediction, label.view(-1,1), reduction='none')\n",
    "        \n",
    "        eval_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        eval_MSE_list.extend(MSE.data.squeeze().cpu().numpy())\n",
    "    return np.array(eval_MAE_list).mean(), np.array(eval_MSE_list).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_MAE, valid_MSE = eval(smiles_list[valid_fold[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | loss | train MSE |  valid MSE |  time \n",
      "  0   | 1.3945  | 1.3209  | 1.3415  |  0 hr 00 min \n",
      "  1   | 1.2612  | 1.2100  | 1.2760  |  0 hr 00 min \n",
      "  2   | 1.2459  | 1.1717  | 1.2378  |  0 hr 00 min \n",
      "  3   | 1.2110  | 1.1991  | 1.3137  |  0 hr 01 min \n",
      "  4   | 1.1984  | 1.1708  | 1.2486  |  0 hr 01 min \n",
      "  5   | 1.1901  | 1.0993  | 1.2102  |  0 hr 01 min \n",
      "  6   | 1.1398  | 1.1038  | 1.2108  |  0 hr 02 min \n",
      "  7   | 1.1318  | 1.1827  | 1.2808  |  0 hr 02 min \n",
      "  8   | 1.1409  | 1.1554  | 1.3294  |  0 hr 02 min \n",
      "  9   | 1.1335  | 1.0715  | 1.1708  |  0 hr 02 min \n",
      " 10   | 1.1244  | 1.1051  | 1.2419  |  0 hr 02 min \n",
      " 11   | 1.1171  | 1.0586  | 1.2047  |  0 hr 03 min \n",
      " 12   | 1.0897  | 1.1130  | 1.2359  |  0 hr 03 min \n",
      " 13   | 1.0805  | 1.0777  | 1.1672  |  0 hr 03 min \n",
      " 14   | 1.0638  | 1.0717  | 1.2156  |  0 hr 03 min \n",
      " 15   | 1.0488  | 0.9718  | 1.1376  |  0 hr 04 min \n",
      " 16   | 1.0308  | 1.0396  | 1.1820  |  0 hr 04 min \n",
      " 17   | 1.0288  | 0.9546  | 1.1518  |  0 hr 04 min \n",
      " 18   | 1.0028  | 0.9323  | 1.1230  |  0 hr 04 min \n",
      " 19   | 1.0139  | 0.9701  | 1.1655  |  0 hr 05 min \n",
      " 20   | 1.0194  | 0.9472  | 1.1547  |  0 hr 05 min \n",
      " 21   | 1.0585  | 1.0202  | 1.1911  |  0 hr 05 min \n",
      " 22   | 1.0572  | 0.9706  | 1.1261  |  0 hr 05 min \n",
      " 23   | 1.0034  | 0.9486  | 1.1390  |  0 hr 06 min \n",
      " 24   | 1.0103  | 0.9224  | 1.1570  |  0 hr 06 min \n",
      " 25   | 0.9759  | 0.8843  | 1.1758  |  0 hr 06 min \n",
      " 26   | 0.9513  | 0.9036  | 1.1912  |  0 hr 06 min \n",
      " 27   | 0.9421  | 0.8747  | 1.1574  |  0 hr 07 min \n",
      " 28   | 0.9475  | 0.8571  | 1.1463  |  0 hr 07 min \n",
      " 29   | 0.9172  | 0.8246  | 1.1391  |  0 hr 07 min \n",
      " 30   | 0.9139  | 0.8323  | 1.1318  |  0 hr 07 min \n",
      " 31   | 0.8995  | 0.8502  | 1.1637  |  0 hr 08 min \n",
      " 32   | 0.9135  | 0.8264  | 1.0931  |  0 hr 08 min \n",
      " 33   | 0.9052  | 0.8108  | 1.0905  |  0 hr 08 min \n",
      " 34   | 0.8774  | 0.7723  | 1.1494  |  0 hr 08 min \n",
      " 35   | 0.8647  | 0.7886  | 1.1339  |  0 hr 09 min \n",
      " 36   | 0.8738  | 0.7860  | 1.1227  |  0 hr 09 min \n",
      " 37   | 0.8655  | 0.7621  | 1.1375  |  0 hr 09 min \n",
      " 38   | 0.8318  | 0.7464  | 1.1079  |  0 hr 09 min \n",
      " 39   | 0.8260  | 0.7339  | 1.1264  |  0 hr 10 min \n",
      " 40   | 0.8356  | 0.7321  | 1.1502  |  0 hr 10 min \n",
      " 41   | 0.8166  | 0.7335  | 1.1408  |  0 hr 10 min \n",
      " 42   | 0.7924  | 0.7099  | 1.1477  |  0 hr 10 min \n",
      " 43   | 0.7947  | 0.6781  | 1.1325  |  0 hr 10 min \n",
      " 44   | 0.7934  | 0.6753  | 1.1592  |  0 hr 11 min \n",
      " 45   | 0.7790  | 0.6960  | 1.1659  |  0 hr 11 min \n",
      " 46   | 0.7748  | 0.6730  | 1.1487  |  0 hr 11 min \n",
      " 47   | 0.7535  | 0.6674  | 1.1093  |  0 hr 11 min \n",
      " 48   | 0.7528  | 0.6658  | 1.1539  |  0 hr 12 min \n",
      " 49   | 0.7735  | 0.6814  | 1.1850  |  0 hr 12 min \n",
      " 50   | 0.7506  | 0.6300  | 1.1553  |  0 hr 12 min \n",
      " 51   | 0.7278  | 0.6263  | 1.1298  |  0 hr 12 min \n",
      " 52   | 0.7389  | 0.6378  | 1.1461  |  0 hr 12 min \n",
      " 53   | 0.7414  | 0.6428  | 1.1777  |  0 hr 13 min \n",
      " 54   | 0.7395  | 0.6207  | 1.1727  |  0 hr 13 min \n",
      " 55   | 0.7149  | 0.6099  | 1.2057  |  0 hr 13 min \n",
      " 56   | 0.7126  | 0.6294  | 1.1441  |  0 hr 13 min \n",
      " 57   | 0.7043  | 0.6012  | 1.2054  |  0 hr 13 min \n",
      " 58   | 0.6893  | 0.5789  | 1.2147  |  0 hr 14 min \n",
      " 59   | 0.7139  | 0.6001  | 1.1978  |  0 hr 14 min \n",
      " 60   | 0.6914  | 0.5870  | 1.1663  |  0 hr 14 min \n",
      " 61   | 0.6816  | 0.5684  | 1.1504  |  0 hr 14 min \n",
      " 62   | 0.6739  | 0.5729  | 1.1306  |  0 hr 15 min \n",
      " 63   | 0.6696  | 0.5240  | 1.1539  |  0 hr 15 min \n",
      " 64   | 0.6471  | 0.5305  | 1.1701  |  0 hr 15 min \n",
      " 65   | 0.6420  | 0.5450  | 1.1574  |  0 hr 16 min \n",
      " 66   | 0.6468  | 0.5303  | 1.2219  |  0 hr 16 min \n",
      " 67   | 0.6282  | 0.5045  | 1.1881  |  0 hr 16 min \n",
      " 68   | 0.6260  | 0.5276  | 1.1780  |  0 hr 16 min \n",
      " 69   | 0.6377  | 0.5130  | 1.2156  |  0 hr 17 min \n",
      " 70   | 0.6088  | 0.4933  | 1.1918  |  0 hr 17 min \n",
      " 71   | 0.6172  | 0.4751  | 1.2166  |  0 hr 17 min \n",
      " 72   | 0.5803  | 0.4780  | 1.1683  |  0 hr 18 min \n",
      " 73   | 0.5994  | 0.4556  | 1.2076  |  0 hr 18 min \n",
      " 74   | 0.5815  | 0.4788  | 1.2308  |  0 hr 18 min \n",
      " 75   | 0.5631  | 0.4220  | 1.2253  |  0 hr 18 min \n",
      " 76   | 0.5684  | 0.4484  | 1.1924  |  0 hr 19 min \n",
      " 77   | 0.5599  | 0.4281  | 1.2134  |  0 hr 19 min \n",
      " 78   | 0.5490  | 0.4177  | 1.2284  |  0 hr 19 min \n",
      " 79   | 0.5422  | 0.4149  | 1.2154  |  0 hr 19 min \n",
      " 80   | 0.5653  | 0.4378  | 1.2404  |  0 hr 20 min \n",
      " 81   | 0.5612  | 0.4347  | 1.2052  |  0 hr 20 min \n",
      " 82   | 0.5566  | 0.4080  | 1.1976  |  0 hr 20 min \n",
      " 83   | 0.5389  | 0.4139  | 1.2024  |  0 hr 20 min \n",
      " 84   | 0.5380  | 0.4037  | 1.2841  |  0 hr 21 min \n",
      " 85   | 0.5090  | 0.4132  | 1.2228  |  0 hr 21 min \n",
      " 86   | 0.4987  | 0.3853  | 1.2585  |  0 hr 21 min \n",
      " 87   | 0.5036  | 0.4023  | 1.2687  |  0 hr 21 min \n",
      " 88   | 0.5134  | 0.3963  | 1.2320  |  0 hr 21 min \n",
      " 89   | 0.5042  | 0.3728  | 1.2508  |  0 hr 22 min \n",
      " 90   | 0.4992  | 0.3895  | 1.2408  |  0 hr 22 min \n",
      " 91   | 0.4773  | 0.3552  | 1.1978  |  0 hr 22 min \n",
      " 92   | 0.4664  | 0.3730  | 1.2195  |  0 hr 22 min \n",
      " 93   | 0.4699  | 0.3436  | 1.2601  |  0 hr 23 min \n",
      " 94   | 0.4704  | 0.3566  | 1.2521  |  0 hr 23 min \n",
      " 95   | 0.4646  | 0.3549  | 1.2438  |  0 hr 23 min \n",
      " 96   | 0.4505  | 0.3286  | 1.2383  |  0 hr 23 min \n",
      " 97   | 0.4500  | 0.3207  | 1.2625  |  0 hr 24 min \n",
      " 98   | 0.4614  | 0.3237  | 1.2406  |  0 hr 24 min \n",
      " 99   | 0.4439  | 0.3047  | 1.2285  |  0 hr 24 min \n",
      " 100  | 0.4302  | 0.3097  | 1.2419  |  0 hr 25 min \n",
      " 101  | 0.4338  | 0.3330  | 1.2523  |  0 hr 25 min \n",
      " 102  | 0.4309  | 0.3033  | 1.2492  |  0 hr 25 min \n",
      " 103  | 0.4366  | 0.3252  | 1.2225  |  0 hr 26 min \n",
      " 104  | 0.4371  | 0.3217  | 1.2779  |  0 hr 26 min \n",
      " 105  | 0.4399  | 0.3189  | 1.2693  |  0 hr 26 min \n",
      " 106  | 0.4313  | 0.3024  | 1.2888  |  0 hr 26 min \n",
      " 107  | 0.4192  | 0.2859  | 1.2360  |  0 hr 27 min \n",
      " 108  | 0.4002  | 0.2896  | 1.2421  |  0 hr 27 min \n",
      " 109  | 0.4158  | 0.2781  | 1.2223  |  0 hr 27 min \n",
      " 110  | 0.4079  | 0.2892  | 1.2286  |  0 hr 28 min \n",
      " 111  | 0.4345  | 0.3356  | 1.2307  |  0 hr 28 min \n",
      " 112  | 0.4353  | 0.3286  | 1.2633  |  0 hr 28 min \n",
      " 113  | 0.4241  | 0.2893  | 1.2728  |  0 hr 28 min \n",
      " 114  | 0.4209  | 0.2904  | 1.2619  |  0 hr 28 min \n",
      " 115  | 0.4150  | 0.2822  | 1.2587  |  0 hr 29 min \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-221038e7fafe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtraine_MAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_MSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mvalid_MAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_MSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-e58947cd9c63>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(smiles_list)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_param ={}\n",
    "best_param[\"train_epoch\"] = 0\n",
    "best_param[\"valid_epoch\"] = 0\n",
    "best_param[\"train_MSE\"] = 9e8\n",
    "best_param[\"valid_MSE\"] = 9e8\n",
    "\n",
    "log = Logger()\n",
    "log.open(f'{prefix_filename}_start_time{start_time}.txt')\n",
    "\n",
    "f = '{:^5} | {:^7.4f} | {:^7.4f} | {:^7.4f} | {:^7} \\n'\n",
    "log.write('epoch | loss | train MSE |  valid MSE |  time \\n')\n",
    "start = timer()\n",
    "fold_index = 0\n",
    "# losses = 10000\n",
    "for epoch in range(800):\n",
    "    \n",
    "    losses = train(smiles_list[train_fold[fold_index]])\n",
    "    traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "    valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "#     if train_MSE < best_param[\"train_MSE\"]:\n",
    "#         best_param[\"train_epoch\"] = epoch\n",
    "#         best_param[\"train_MSE\"] = train_MSE\n",
    "#     if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "#         best_param[\"valid_epoch\"] = epoch\n",
    "#         best_param[\"valid_MSE\"] = valid_MSE\n",
    "#         if valid_MSE < 0.35:\n",
    "#              torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "#     if (epoch - best_param[\"train_epoch\"] >10) and (epoch - best_param[\"valid_epoch\"] >18):        \n",
    "#         break\n",
    "        \n",
    "    timing = time_to_str((timer() - start), 'min')  \n",
    "    log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for e in range(20):\n",
    "#     losses = train(smiles_list[valid_fold[fold_index]])\n",
    "#     print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_fold[fold_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "best_model = torch.load('saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(best_param[\"valid_epoch\"])+'.pt')     \n",
    "\n",
    "best_model_dict = best_model.state_dict()\n",
    "best_model_wts = copy.deepcopy(best_model_dict)\n",
    "\n",
    "model.load_state_dict(best_model_wts)\n",
    "(best_model.align[0].weight == model.align[0].weight).all()\n",
    "test_MAE, test_MSE = eval(model, test_df)\n",
    "print(\"best epoch:\",best_param[\"test_epoch\"],\"\\n\",\"test MSE:\",test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
